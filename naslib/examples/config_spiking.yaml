# random seed
seed: 0

# re, bananas, npenas, ls, rs
optimizer: re

# nasbench101, nasbench201, darts, nlp, 
# transbench101_micro, transbench101_macro, asr
search_space: darts

# nasbench201 datasets: cifar10, cifar100, ImageNet16-120
# transbench101 datasets: class_scene, class_object, 
# jigsaw, room_layout, segmentsemantic, normal, autoencoder
dataset: shd

# output results to this directory
out_dir: run

# parameters for the optimizers
search:
  # for bohb
  budgets: 50000000
  checkpoint_freq: 1000
  fidelity: 108 
  
  # for all optimizers
  epochs: 100

  # for bananas and npenas, choose one predictor 
  # out of the 16 model-based predictors
  predictor_type: var_sparse_gp
  
  # number of initial architectures
  num_init: 10  

  # BANANAS
  k: 10
  num_ensemble: 3
  acq_fn_type: its
  acq_fn_optimization: mutation
  encoding_type: adjacency_one_hot
  num_arches_to_mutate: 5
  max_mutations: 1
  num_candidates: 200
  
  # jacov data loader
  batch_size: 256
  data_size: 25000
  cutout: False
  cutout_length: 16
  cutout_prob: 1.0
  train_portion: 0.7
  
  # other params
  debug_predictor: False
  sample_size: 10
  population_size: 30





# seed: 0
# optimizer: oneshot
# search_space: darts
# dataset: shd
# out_dir: run

# experiment_type: single
# predictor: oneshot
# test_size: 200
# train_size_list: [8, 12]
# train_size_single: 2
# fidelity_list: [5]
# fidelity_single: 5

# search:
#   checkpoint_freq: 1000
#   epochs: 50
#   fidelity: -1

#   # GDAS
#   tau_max: 10
#   tau_min: 0.1

#   # RE
#   sample_size: 10
#   population_size: 30
  
#   # LS
#   num_init: 10
  
#   # BANANAS
#   k: 10
#   num_ensemble: 3
#   acq_fn_type: its
#   acq_fn_optimization: mutation
#   encoding_type: path
#   num_arches_to_mutate: 2
#   max_mutations: 1
#   num_candidates: 100
  
#   # BP
#   predictor_type: oneshot
#   debug_predictor: False


#   # additional params
  batch_size: 64
  learning_rate: 0.025
  learning_rate_min: 0.001
  momentum: 0.9
  weight_decay: 0.0003
  warm_start_epochs: 0
  grad_clip: 5
#   train_portion: 0.9
#   data_size: 25000

#   cutout: False
#   cutout_length: 16
#   cutout_prob: 1.0
#   drop_path_prob: 0.0

#   unrolled: False
  arch_learning_rate: 0.0003
  arch_weight_decay: 0.001
#   output_weights: True
  

evaluation:
  checkpoint_freq: 5000
  batch_size: 96
  learning_rate: 0.025
  learning_rate_min: 0.00
  momentum: 0.9
  weight_decay: 0.0003
  epochs: 600
  warm_start_epochs: 0
  grad_clip: 5
  train_portion: 1.
  data_size: 50000

  cutout: True
  cutout_length: 16
  cutout_prob: 1.0
  drop_path_prob: 0.2
  auxiliary_weight: 0.4
